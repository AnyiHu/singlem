#!/usr/bin/env python

import subprocess
import tempfile
import re
import shutil
import csv


__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2015"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3"
__maintainer__ = "Ben Woodcroft"
__email__ = "b.woodcroft near uq.edu.au"
__status__ = "Development"

import argparse
import logging
import sys
import os
import tempdir

try:
    import singlem.singlem
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)),'..'))
from singlem.singlem import MetagenomeOtuFinder, Sequence, \
    HmmDatabase, SeqReader, TaxonomyFile
import singlem




def seqs(args):
    # Read in the fasta Alignment
    protein_alignment = protein_alignment_from_alignment_file(args.alignment)
    logging.info("Read in %i aligned protein sequences e.g. %s %s" % (len(protein_alignment),
                                                          protein_alignment[0].name,
                                                          protein_alignment[0].seq
                                                          ))

    # Read in the original nucleotide sequences
    nucleotide_sequences = read_nucleotide_sequences(args.reads)
    eg_name = nucleotide_sequences.keys()[0]
    logging.info("Read in %i nucleotide sequences e.g. %s %s" % (len(nucleotide_sequences),
                                                          eg_name,
                                                          nucleotide_sequences[eg_name]
                                                          ))
    if args.start_position:
        best_position = args.start_position - 1
    else:
        best_position = None
    aligned_sequences = MetagenomeOtuFinder().find_windowed_sequences(protein_alignment,
                                                nucleotide_sequences,
                                                args.window_size,
                                                best_position)
    logging.info("Printing %i aligned sequences" % len(aligned_sequences))
    print '\n'.join(aligned_sequences)


def protein_alignment_from_alignment_file(alignment_file):
    protein_alignment = []
    for name, seq, _ in SeqReader().readfq(open(alignment_file)):
        protein_alignment.append(Sequence(name, seq))
    logging.info("Read in %i aligned protein sequences e.g. %s %s" % (len(protein_alignment),
                                                          protein_alignment[0].name,
                                                          protein_alignment[0].seq
                                                          ))
    return protein_alignment

def read_nucleotide_sequences(nucleotide_file):
    nucleotide_sequences = {}
    for name, seq, _ in SeqReader().readfq(open(nucleotide_file)):
        nucleotide_sequences[name] = seq
    return nucleotide_sequences


def align_proteins_to_hmm(proteins_file, hmm_file):
    '''hmmalign proteins to hmm, and return an alignment object'''
    
    with tempfile.NamedTemporaryFile(prefix="singlem", suffix=".fasta") as f:
        cmd = "hmmalign %s %s |seqmagick convert --input-format stockholm - %s" % (hmm_file,
                                                          proteins_file,
                                                          f.name)
        logging.debug("Running cmd: %s" % cmd)
        subprocess.check_call(cmd, shell=True)
        return protein_alignment_from_alignment_file(f.name)

def pipe(args):
    hmms = HmmDatabase()
    forward_read_files = args.forward
    output_otu_table = args.otu_table
    num_threads = args.threads
    
    using_temporary_working_directory = args.working_directory is None
    if using_temporary_working_directory:
        tmp = tempdir.TempDir()
        working_directory = tmp.name
    else:
        working_directory = args.working_directory
        if os.path.exists(working_directory):
            if args.force:
                logging.info("Overwriting directory %s" % working_directory)
                shutil.rmtree(working_directory)
            else:
                raise Exception("Working directory '%s' already exists, not continuing" % working_directory)
        os.mkdir(working_directory)
    logging.debug("Using working directory %s" % working_directory)
    
    if not args.previous_graftm_search_directory:
        graftm_search_directory = os.path.join(working_directory, 'graftm_search')
    
        # run graftm across all the HMMs
        logging.info("Using as input %i different forward read sets e.g. %s" % (len(args.forward),
                                                                            args.forward[0]))
        cmd = "graftM graft --threads %i --forward %s "\
            "--search_hmm_files %s --search_and_align_only "\
            "--output_directory %s --aln_hmm_file %s --verbosity 2 "\
            "--input_sequence_type nucleotide"\
            % (
                                num_threads,
                                ' '.join(forward_read_files),
                                ' '.join(hmms.hmm_paths()),
                                graftm_search_directory,
                                hmms.hmm_paths()[0]
                                )
        logging.info("Running GraftM to find particular reads..")
        logging.debug("Running cmd %s" % cmd)
        subprocess.check_call(cmd, shell=True)
        logging.info("Finished running GraftM search phase")
    else:
        graftm_search_directory = args.previous_graftm_search_directory
        logging.info("Using existing graftM directory %s" % args.previous_graftm_search_directory)

    # Get the names of the samples from the graftm directory
    sample_names = [f for f in os.listdir(graftm_search_directory) \
                    if os.path.isdir(os.path.join(graftm_search_directory, f))]
    logging.debug("Recovered %i samples from graftm search output e.g. %s" \
                 % (len(sample_names), sample_names[0]))

    # runs graftm for each of the HMMs doing the actual alignments, for each
    # of the input sequences
    if args.previous_graftm_placement_directory:
        graftm_align_directory_base = args.previous_graftm_placement_directory
    else:
        graftm_align_directory_base = os.path.join(working_directory, 'graftm_aligns')
        os.mkdir(graftm_align_directory_base)
        with tempfile.NamedTemporaryFile() as gpkgs_file:
            gpkgs_file.write("\n".join(hmms.gpkg_paths()))
            gpkgs_file.flush()
            with tempfile.NamedTemporaryFile() as samples_file:
                samples_file.write("\n".join(sample_names))
                samples_file.flush()
    
                cmd = "parallel -j1 graftM graft --threads %i --verbosity 2 "\
                 "--forward %s/{1}/{1}_hits.fa "\
                 "--graftm_package {2} --output_directory %s/{1}_vs_{2/} "\
                 "--input_sequence_type nucleotide "\
                 "::: `cat %s` ::: `cat %s`" % \
                  (
                   args.threads,
                   graftm_search_directory,
                   graftm_align_directory_base,
                   samples_file.name,
                   gpkgs_file.name
                   )
                logging.info("Running alignments/placement in GraftM..")
                logging.debug("Running cmd: %s" % cmd)
                subprocess.check_call(cmd, shell=True)

    # get the sequences out for each of them
    with open(output_otu_table,'w') as output:
        un_hitify_re = re.compile(r'(.*)_vs_(.*)')
        for f in os.listdir(graftm_align_directory_base):
            if not os.path.isdir(os.path.join(graftm_align_directory_base,f)): continue
            unhitted = un_hitify_re.match(f)
            if not unhitted:
                raise Exception("Unexpected graftm sequence name %s" % f)
            sample_name = unhitted.groups(0)[0]
            hmm_basename = unhitted.groups(0)[1]

            hits_name = "%s_hits" % sample_name
            base_dir = os.path.join(graftm_align_directory_base,
                                          f,
                                          hits_name)
            
            # Need to re-align the sequences because GraftM removes columns
            # from the alignment, and we need all of them to be able to align
            # with nucleotide sequences
            proteins_file = os.path.join(base_dir, "%s_hits_orf.fa" % sample_name)
            protein_alignment = align_proteins_to_hmm(proteins_file,
                                                      hmms.hmms_and_positions[hmm_basename].hmm_path()
                                                      )
            nucleotide_file = os.path.join(base_dir, "%s_hits_hits.fa" % sample_name)
            nucleotide_sequences = read_nucleotide_sequences(nucleotide_file)
            taxonomies = TaxonomyFile(os.path.join(base_dir, "%s_hits_read_tax.tsv" % sample_name))

            aligned_seqs = MetagenomeOtuFinder().find_windowed_sequences(protein_alignment,
                                                    nucleotide_sequences,
                                                    20,
                                                    hmms.hmms_and_positions[hmm_basename].best_position)
            logging.debug("Found %i sequences for hmm %s, sample %s" % (len(aligned_seqs),
                                                                        hmm_basename,
                                                                        sample_name))
            
            # convert to OTU table, output
            for seq, count, taxonomy, names in seqs_to_counts_and_taxonomy(aligned_seqs,
                                                                    taxonomies
                                                                    ):
                to_print = [hmm_basename,
                                sample_name,
                                seq,
                                str(count),
                                taxonomy]
                if args.output_read_names:
                    to_print.append(' '.join(names))
                output.write("\t".join(to_print) + "\n")
                
    # Be clean.
    if using_temporary_working_directory:
        tmp.dissolve()


def seqs_to_counts_and_taxonomy(sequences, taxonomies):
    '''given an array of Sequence objects, and hash of taxonomy file,
    yield over the counts of the aggregated sequences and corresponding
    median taxonomies
    '''
    counts = {}
    taxonomy_aggregates = {}
    names = {}
    for s in sequences:
        tax = taxonomies[s.name]
        try:
            counts[s.seq] += 1
            taxonomy_aggregates[s.seq].append(tax)
            names[s.seq].append(s.name)
        except KeyError:
            counts[s.seq] = 1
            taxonomy_aggregates[s.seq] = [tax]
            names[s.seq] = [s.name]
    for seq, aggregate in taxonomy_aggregates.iteritems():
        yield seq, counts[seq], median_taxonomy(aggregate), names[seq]

def median_taxonomy(taxonomies):
    levels_to_counts = []
    for tax_string in taxonomies:
        for i, tax in enumerate(tax_string.split('; ')):
            if i >= len(levels_to_counts):
                levels_to_counts.append({})
            try:
                levels_to_counts[i][tax] += 1
            except KeyError:
                levels_to_counts[i][tax] = 1
                
                
    median_tax = []
    for level_counts in levels_to_counts:
        max_count = 0
        max_tax = None
        for tax, count in level_counts.iteritems():
            if count > max_count:
                max_count = count
                max_tax = tax
        if float(max_count) / len(taxonomies) >= 0.5:
            median_tax.append(max_tax)
        else:
            break
    return '; '.join(median_tax)
    
def makedb(args):
    otu_file = args.otu_table
    fasta_file = "%s.fasta" % otu_file
    with open(fasta_file, 'w') as fasta:
        for i, row in enumerate(csv.reader(open(otu_file), delimiter="\t")):
            fasta.write('|'.join([">gnl",
                                  str(i+1),
                                  row[0],row[1],row[3]]))
            fasta.write(' '+row[4])
            if len(row)>5:
                fasta.write(' '+'|'.join(row[5:]))
            fasta.write("\n%s\n" % row[2])
    cmd = "makeblastdb -in '%s' -dbtype nucl -parse_seqids" % fasta.name
    logging.debug("Running cmd: %s" % cmd)
    subprocess.check_call(cmd, shell=True)
    
def query(args):
    db = args.db
    query_sequence = args.query_sequence
    max_target_seqs = args.max_hits
    max_divergence = args.max_divergence
    
    # blast the query against the database, output as csv
    with tempfile.NamedTemporaryFile(prefix='singlem_query') as infile:
        infile.write(">query\n")
        infile.write(query_sequence+"\n")
        infile.flush()
        
        distances_and_names = []
        
        cmd = "blastn -task blastn -query '%s' -db '%s' -outfmt 6 -max_target_seqs %i" %\
            (infile.name, db, max_target_seqs)
        logging.debug("Running cmd %s" % cmd)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        for line in iter(proc.stdout.readline,''):
            _, sseqid, _, _, mismatch, gapopen, qstart, qend = line.strip().split("\t")[:8]
            query_length = len(query_sequence.replace('-',''))
            num_hits = int(sseqid.split("|")[4])
            divergence = int(mismatch) + int(gapopen) + (int(qstart)-1) + (query_length-int(qend))
            if divergence <= max_divergence:
                distances_and_names.append([divergence, num_hits, sseqid])
        
        # sort the array according to increasing divergence, then increasing number of hits
        distances_and_names.sort(key=lambda x: [x[0],-x[1]])
        
        # print out array
        print "\t".join(['divergence','num_hits','sample','marker'])
        for hit in distances_and_names:
            spl = hit[2].split('|')
            print "\t".join([str(hit[0]),str(hit[1]),spl[3],spl[2]])
    

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--debug', help='output debug information', action="store_true")
    parser.add_argument('--version', help='output version information and quit',  action='version', version=singlem.__version__)
    parser.add_argument('--quiet', help='only output errors', action="store_true")
    subparsers = parser.add_subparsers(help="--", dest='subparser_name')

    pipe_parser = subparsers.add_parser('pipe',
                                        description='From raw reads to OTU table',
                                        epilog=__author__)
    pipe_parser.add_argument('--forward', nargs='+', metavar='sequence_file', help='forward nucleotide sequences to be searched')
    pipe_parser.add_argument('--threads', type=int, metavar='num_threads', help='number of CPUS to use', default=1)
    pipe_parser.add_argument('--otu_table', metavar='filename', help='output OTU table', required=True)
    pipe_parser.add_argument('--working_directory', metavar='directory', help='work in this directory (default: a temporary directory)')
    pipe_parser.add_argument('--force', action='store_true', help='overwrite working directory if required')
    pipe_parser.add_argument('--previous_graftm_search_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--previous_graftm_placement_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--output_read_names', action='store_true', help='give the names of the reads for each OTU in the table (mainly for debug purposes)', default=False)

    seqs_parser = subparsers.add_parser('seqs',
                                        description='From GraftM alignment to aligned nucleotide sequences',
                                        epilog=__author__)
    seqs_parser.add_argument('--alignment', metavar='aligned_fasta', help="Protein sequences hmmaligned and converted to fasta format with seqmagick", required=True)
    seqs_parser.add_argument('--reads', metavar='raw_reads', help='Unaligned nucleotide sequences that were translated into the protein sequences', required=True)
    seqs_parser.add_argument('--window_size', metavar='bp', help='Number of base pairs to use in continuous window', default=20, type=int)
    seqs_parser.add_argument('--start_position', metavar='bp', help='Start the window at the position in the alignment (1-based index) [default: pick one automatically]', type=int)
    
    makedb_parser = subparsers.add_parser('makedb',
                                        description='From aligned nucleotide sequences to searchable database',
                                        epilog=__author__)
    makedb_parser.add_argument('--otu_table', help="Output from 'pipe' mode", required=True)
    
    makedb_parser = subparsers.add_parser('query',
                                        description='Find closely related sequences in a singlem database',
                                        epilog=__author__)
    makedb_parser.add_argument('--query_sequence', metavar='sequence', help="Sequence to use as a query", required=True)
    makedb_parser.add_argument('--db', help="Output from 'makedb' mode", required=True)
    makedb_parser.add_argument('--max_divergence', metavar='INT', help="Report sequences less than or equal to this divergence", default=4, type=int)
    makedb_parser.add_argument('--max_hits', help="--max_target_seqs parameter for blast", default=500, type=int)
    

    args = parser.parse_args()
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    elif args.quiet:
        logging.basicConfig(level=logging.ERROR)
    else:
        logging.basicConfig(level=logging.INFO)

    if args.subparser_name == 'seqs':
        seqs(args)
    elif args.subparser_name=='pipe':
        pipe(args)
    elif args.subparser_name=='makedb':
        makedb(args)
    elif args.subparser_name=='query':
        query(args)
    else:
        raise Exception()















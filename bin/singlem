#!/usr/bin/env python

__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2015"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3+"
__maintainer__ = "Ben Woodcroft"
__email__ = "b.woodcroft near uq.edu.au"
__status__ = "Development"

import argparse
import logging
import sys
import os
import tempfile
import subprocess
import math

sys.path = [os.path.join(os.path.dirname(os.path.realpath(__file__)),'..')] + sys.path

import singlem.sequence_database
import singlem.query_formatters
import singlem.pipe as pipe
from singlem.summariser import Summariser
from singlem.query_formatters import NameSequenceQueryDefinition,\
    NamedQueryDefinition
from singlem.strain_summariser import StrainSummariser
from singlem.sequence_classes import SeqReader
from singlem.metagenome_otu_finder import MetagenomeOtuFinder
from singlem.package_creator import PackageCreator
from singlem.otu_table_collection import OtuTableCollection
from singlem.clusterer import Clusterer
from singlem.appraiser import Appraiser

ALMOST_90_PERCENT_ON_60BP = round(math.floor(0.97*60)/60, 3)

def seqs(args):
    # Read in the fasta Alignment
    protein_alignment = SeqReader().protein_alignment_from_alignment_file(args.alignment)
    logging.info("Read in %i aligned protein sequences e.g. %s %s" % (len(protein_alignment),
                                                          protein_alignment[0].name,
                                                          protein_alignment[0].seq
                                                          ))

    # Read in the original nucleotide sequences
    nucleotide_sequences = SeqReader().read_nucleotide_sequences(args.reads)
    eg_name = nucleotide_sequences.keys()[0]
    logging.info("Read in %i nucleotide sequences e.g. %s %s" % (len(nucleotide_sequences),
                                                          eg_name,
                                                          nucleotide_sequences[eg_name]
                                                          ))
    if args.start_position:
        best_position = args.start_position - 1
    else:
        best_position = None
    aligned_sequences = MetagenomeOtuFinder().find_windowed_sequences(protein_alignment,
                                                nucleotide_sequences,
                                                args.window_size,
                                                best_position)
    logging.info("Printing %i aligned sequences" % len(aligned_sequences))
    print '\n'.join((s.aligned_sequence for s in aligned_sequences))



def query(args):
    db = singlem.sequence_database.SequenceDatabase.acquire(args.db)
    query_sequence = args.query_sequence
    max_target_seqs = args.max_hits
    max_divergence = args.max_divergence
    output_style = args.otu_table_type
    query_otu_table = args.query_otu_table
    query_fasta = args.query_fasta
    if (query_otu_table and query_sequence) or \
        (query_otu_table and query_fasta) or \
        (query_sequence and query_fasta):
        raise Exception("Only one of --query_fasta, --query_otu_table and --query_sequence is allowable")

    if query_sequence:
        query_names = ['unnamed_sequence']
        query_sequences = [query_sequence]
    elif query_otu_table:
        query_names = []
        query_sequences = []
        otus = OtuTableCollection()
        otus.add_otu_table(open(query_otu_table))
        for e in otus:
            query_sequences.append(e.sequence)
            query_names.append(';'.join([e.sample_name,e.marker]))
    elif query_fasta:
        query_names = []
        query_sequences = []
        for name, seq, _ in SeqReader().readfq(open(query_fasta)):
            query_names.append(name)
            query_sequences.append(seq)
    else:
        raise Exception("No query option specified, cannot continue")

    # blast the query against the database, output as csv
    found_distances_and_names = []
    found_query_names = []
    found_query_sequences = []
    with tempfile.NamedTemporaryFile(prefix='singlem_query') as infile:
        for i, sequence in enumerate(query_sequences):
            infile.write(">%i\n" % i)
            infile.write(sequence+"\n")
        infile.flush()

        cmd = "blastn -task blastn -query '%s' -db '%s' -outfmt 6 -max_target_seqs %i" %\
            (infile.name, db.sequences_fasta_file, max_target_seqs)
        logging.debug("Running cmd %s" % cmd)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)

        last_index = None
        last_differences_and_names = []
        for line in iter(proc.stdout.readline,''):
            qseqid, sseqid, _, _, mismatch, gapopen, qstart, qend = line.strip().split("\t")[:8]
            index = int(qseqid)
            if last_index is None:
                # first run through loop
                last_index = index
            elif index != last_index:
                # changed query sequences, move to the next one
                found_query_names.append(query_names[last_index])
                found_query_sequences.append(query_sequences[last_index])
                found_distances_and_names.append(last_differences_and_names)
                last_index = index
                last_differences_and_names = []

            #TODO: check we haven't come up against the max_target_seqs barrier
            query_length = len(query_sequences[index].replace('-',''))
            subject = db.extract_sequence_by_sseqid(sseqid)
            divergence = int(mismatch) + int(gapopen) + (int(qstart)-1) + (query_length-int(qend))
            if divergence <= max_divergence:
                last_differences_and_names.append([divergence, subject])

        if last_index is not None:
            # finish off the last chunk
            found_query_names.append(query_names[last_index])
            found_query_sequences.append(query_sequences[last_index])
            found_distances_and_names.append(last_differences_and_names)

    if query_fasta:
        namedef = NamedQueryDefinition(found_query_names)
    else:
        namedef = NameSequenceQueryDefinition(found_query_names, found_query_sequences)

    if output_style == 'sparse':
        formatter = singlem.query_formatters.SparseResultFormatter(namedef, found_distances_and_names)
    elif output_style == 'dense':
        formatter = singlem.query_formatters.DenseResultFormatter(namedef, found_distances_and_names)
    else:
        raise Exception()
    formatter.write(sys.stdout)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--debug', help='output debug information', action="store_true")
    parser.add_argument('--version', help='output version information and quit',  action='version', version=singlem.__version__)
    parser.add_argument('--quiet', help='only output errors', action="store_true")
    subparsers = parser.add_subparsers(title="Sub-commands", dest='subparser_name')

    temp_description = 'From raw reads to OTU table'
    pipe_parser = subparsers.add_parser('pipe',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    pipe_parser.add_argument('--sequences', nargs='+', metavar='sequence_file', help='forward nucleotide sequences to be searched')
    pipe_parser.add_argument('--threads', type=int, metavar='num_threads', help='number of CPUS to use', default=1)
    pipe_parser.add_argument('--otu_table', metavar='filename', help='output OTU table')
    pipe_parser.add_argument('--archive_otu_table', metavar='filename', help='output OTU table in archive form for making DBs etc.')
    pipe_parser.add_argument('--working_directory', metavar='directory', help='work in this directory (default: a temporary directory)')
    pipe_parser.add_argument('--force', action='store_true', help='overwrite working directory if required')
    pipe_parser.add_argument('--assignment_method', choices=(pipe.PPLACER_ASSIGNMENT_METHOD,
                                                             pipe.DIAMOND_ASSIGNMENT_METHOD,
                                                             pipe.DIAMOND_EXAMPLE_BEST_HIT_ASSIGNMENT_METHOD),
                             help='method of assigning taxonomy to sequences. Using \'%s\' means than an example hit ID is given instead of a taxonomic classification' % pipe.DIAMOND_EXAMPLE_BEST_HIT_ASSIGNMENT_METHOD,
                             default=pipe.PPLACER_ASSIGNMENT_METHOD)
    pipe_parser.add_argument('--output_jplace', metavar='filename', help='Output a jplace format file for each singlem package to a file starting with this string, each with one entry per OTU. Requires \'%s\' as the --assignment_method' % pipe.PPLACER_ASSIGNMENT_METHOD)
    pipe_parser.add_argument('--evalue', help='GraftM e-value cutoff')
    pipe_parser.add_argument('--min_orf_length', metavar='length', help='When predicting ORFs require this many base pairs uninterrupted by a stop codon (default: 96)', type=int, default=96)
    pipe_parser.add_argument('--restrict_read_length', metavar='length', help='Only use this many base pairs at the start of each sequence searched (default: no restriction)', type=int)
    pipe_parser.add_argument('--filter_minimum', metavar='length', help='Ignore reads aligning in less than this many positions to the HMM (default: as per "graftM graft")', type=int)
    pipe_parser.add_argument('--previous_graftm_search_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--previous_graftm_separate_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--output_extras', action='store_true', help='give extra output for each sequence identified (mainly for debug purposes)', default=False)
    pipe_parser.add_argument('--include_inserts', action='store_true', help='print the entirety of the sequences, not just the aligned nucleotides', default=False)
    pipe_parser.add_argument('--bootstrap_contigs', nargs='+', help='assembled contigs/scaffolds to help the HMM find hits in reads (default: do not use)')
    pipe_parser.add_argument('--known_otu_tables', nargs='+', help='OTU tables previously generated that trusted taxonomies for each sequence (default: do not use)')
    pipe_parser.add_argument('--singlem_packages', nargs='+', help='SingleM packages to use (default: use the default set)')

    temp_description = 'From protein alignment to aligned nucleotide sequences'
    seqs_parser = subparsers.add_parser('seqs',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    seqs_parser.add_argument('--alignment', metavar='aligned_fasta', help="Protein sequences hmmaligned and converted to fasta format with seqmagick", required=True)
    seqs_parser.add_argument('--reads', metavar='raw_reads', help='Unaligned nucleotide sequences that were translated into the protein sequences', required=True)
    seqs_parser.add_argument('--window_size', metavar='aa', help='Number of amino acids to use in continuous window', default=20, type=int)
    seqs_parser.add_argument('--start_position', metavar='bp', help='Start the window at the position in the alignment (1-based index) [default: pick one automatically]', type=int)

    temp_description = 'From aligned nucleotide sequences to searchable database'
    makedb_parser = subparsers.add_parser('makedb',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    makedb_parser.add_argument('--archive_otu_tables', nargs='+', help="Make a db from these archive tables")
    makedb_parser.add_argument('--otu_tables', nargs='+', help="Make a db from these OTU tables")
    makedb_parser.add_argument('--db_path', help="Name of database to create e.g. tundra.sdb", required=True)

    temp_description = 'Find closely related sequences in a singlem database'
    makedb_parser = subparsers.add_parser('query',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    makedb_parser.add_argument('--query_sequence', metavar='sequence', help="Sequence to use as a query")
    makedb_parser.add_argument('--query_otu_table', metavar='file', help="Query the database with all sequences in this OTU table")
    makedb_parser.add_argument('--query_fasta', metavar='file', help="Query the database with all sequences in this FASTA file")
    makedb_parser.add_argument('--db', help="Output from 'makedb' mode", required=True)
    makedb_parser.add_argument('--otu_table_type', help="Style of output table", default='sparse', choices=['dense','sparse'])
    makedb_parser.add_argument('--max_divergence', metavar='INT', help="Report sequences less than or equal to this divergence", default=4, type=int)
    makedb_parser.add_argument('--max_hits', help="--max_target_seqs parameter for blast", default=500, type=int)

    temp_description = 'Summarise OTU tables'
    summarise_parser = subparsers.add_parser('summarise',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    summarise_parser.add_argument('--input_archive_otu_tables', nargs='+', help="Summarise these tables")
    summarise_parser.add_argument('--input_otu_tables', nargs='+', help="Summarise these tables")
    summarise_parser.add_argument('--cluster', action='store_true', help="Apply sequence clustering to the OTU table")
    summarise_parser.add_argument('--cluster_id', type=float, help="Sequence clustering identity cutoff if --cluster is used", default=ALMOST_90_PERCENT_ON_60BP)
    summarise_parser.add_argument('--output_otu_table', help="Output combined OTU table to this file")
    summarise_parser.add_argument('--krona', help="Name of krona file to generate")
    summarise_parser.add_argument('--strain_overview_table', help="Name of output strains table to generate")
    summarise_parser.add_argument('--unifrac', help="Output UniFrac format file")
    summarise_parser.add_argument('--clustered_output_otu_table', help="Output an OTU table with extra information about the clusters")
    summarise_parser.add_argument('--taxonomy', help="Restrict analysis to OTUs that have this taxonomy (exact taxonomy or more fully resolved)")
    summarise_parser.add_argument('--rarefied_output_otu_table', help="Output rarefied output OTU table, where each gene and sample combination is rarefied")
    summarise_parser.add_argument('--number_to_choose', type=int, help="Rarefy using this many sequences. Sample/gene combinations with an insufficient number of sequences are ignored with a warning [default: maximal number such that all are sufficient]")
    

    temp_description = 'Create a SingleM-compatible GraftM package'
    create_parser = subparsers.add_parser('create',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    create_parser.add_argument('--input_graftm_package', metavar="PATH", help="input package", required=True)
    create_parser.add_argument('--output_singlem_package', metavar="PATH", help="output package", required=True)
    create_parser.add_argument('--hmm_position', metavar="INTEGER", help="position in the GraftM alignment HMM where the SingleM window starts", required=True, type=int)
    create_parser.add_argument('--force', action='store_true', help='overwrite output path if it already exists')
    
    temp_description = 'How much of the metagenome do the genomes account for?'
    appraise_parser = subparsers.add_parser('appraise',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    appraise_parser.add_argument('--metagenome_otu_tables', nargs='+', help="output of 'pipe' run on metagenomes", required=True)
    appraise_parser.add_argument('--genome_otu_tables', nargs='+', help="output of 'pipe' run on genomes", required=True)
    appraise_parser.add_argument('--cluster', action='store_true', help="use sequence clustering identity to account for genomes that are similar to those found in the metagenome", default=False)
    appraise_parser.add_argument('--accounted_for_otu_table', help="output OTU table of accounted-for populations", default=None)
    appraise_parser.add_argument('--unaccounted_for_otu_table', help="Output OTU table of populations not accounted for", default=None)

    args = parser.parse_args()
    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    if args.subparser_name == 'seqs':
        seqs(args)
    elif args.subparser_name=='pipe':
        if not args.otu_table and not args.archive_otu_table:
            raise Exception("At least one of --otu_table or --archive_otu_table must be specified")
        if args.output_jplace and args.assignment_method != pipe.PPLACER_ASSIGNMENT_METHOD:
            raise Exception("If --output_jplace is specified, then --assignment_method must be set to %s" % pipe.PPLACER_ASSIGNMENT_METHOD)
        singlem.pipe.SearchPipe().run(
            sequences = args.sequences,
            otu_table = args.otu_table,
            archive_otu_table = args.archive_otu_table,
            threads = args.threads,
            bootstrap_contigs = args.bootstrap_contigs,
            known_otu_tables = args.known_otu_tables,
            assignment_method = args.assignment_method,
            output_jplace = args.output_jplace,
            evalue = args.evalue,
            min_orf_length = args.min_orf_length,
            restrict_read_length = args.restrict_read_length,
            filter_minimum = args.filter_minimum,
            output_extras = args.output_extras,
            include_inserts = args.include_inserts,
            working_directory = args.working_directory,
            force = args.force,
            previous_graftm_search_directory = args.previous_graftm_search_directory,
            previous_graftm_separate_directory = args.previous_graftm_separate_directory,
            singlem_packages = args.singlem_packages)

    elif args.subparser_name=='makedb':
        if not args.otu_tables and not args.archive_otu_tables:
            raise Exception("Making a database requires input OTU tables or archive tables")
        otus = OtuTableCollection()
        if args.otu_tables:
            for o in args.otu_tables:
                otus.add_otu_table(open(o))
        if args.archive_otu_tables:
            for o in args.archive_otu_tables:
                otus.add_archive_otu_table(open(o))
        singlem.sequence_database.SequenceDatabase.create_from_otu_table\
            (args.db_path, otus)
    elif args.subparser_name=='query':
        query(args)
    elif args.subparser_name == 'summarise':
        num_output_types = 0
        if args.strain_overview_table: num_output_types += 1
        if args.krona: num_output_types += 1
        if args.unifrac: num_output_types += 1
        if args.output_otu_table: num_output_types += 1
        if args.clustered_output_otu_table: num_output_types += 1
        if args.rarefied_output_otu_table: num_output_types += 1
        if num_output_types != 1:
            raise Exception("Exactly 1 output type must be specified, sorry, %i were provided" % num_output_types)
        if not args.input_otu_tables and not args.input_archive_otu_tables:
            raise Exception("Summary requires input OTU tables or archive tables")
        
        otus = OtuTableCollection()
        otus.set_target_taxonomy_by_string(args.taxonomy)
        if args.input_otu_tables:
            for o in args.input_otu_tables:
                otus.add_otu_table(open(o))
        if args.input_archive_otu_tables:
            for o in args.input_archive_otu_tables:
                otus.add_archive_otu_table(open(o))
                
        if args.cluster:
            logging.info("Clustering OTUs with clustering identity %f.." % args.cluster_id)
            o2 = OtuTableCollection()
            o2.otu_table_objects = [list(Clusterer().each_cluster(otus, args.cluster_id))]
            otus = o2
            logging.info("Finished clustering")

        if args.krona:
            Summariser.summarise(
                table_collection = otus,
                krona_output_prefix = args.krona)
        elif args.unifrac:
            Summariser.write_unifrac_format_file(
                table_collection = otus,
                unifrac_output_prefix = args.unifrac)
        elif args.output_otu_table:
            Summariser.write_otu_table(
                table_collection = otus,
                output_table_io = open(args.output_otu_table,'w'))
        elif args.strain_overview_table:
            StrainSummariser().summarise_strains(
                table_collection = otus,
                output_table_io = open(args.strain_overview_table,'w'))
        elif args.clustered_output_otu_table:
            if not args.cluster:
                raise Exception("If --clustered_output_otu_table is set, then clustering (--cluster) must be applied")
            Summariser.write_clustered_otu_table(
                table_collection = otus,
                output_table_io = open(args.clustered_output_otu_table,'w'))
        elif args.rarefied_output_otu_table:
            Summariser.write_rarefied_otu_table(
                table_collection = otus,
                output_table_io = open(args.rarefied_output_otu_table,'w'),
                number_to_choose = args.number_to_choose)

        else: raise Exception("Programming error")
        logging.info("Finished")
            
    elif args.subparser_name == 'create':
        PackageCreator().create(input_graftm_package = args.input_graftm_package,
                                output_singlem_package = args.output_singlem_package,
                                hmm_position = args.hmm_position,
                                force = args.force)
    elif args.subparser_name == 'appraise':
        appraiser = Appraiser()
        genomes = OtuTableCollection()
        for table in args.genome_otu_tables:
            genomes.add_otu_table(open(table))
        metagenomes = OtuTableCollection()
        for table in args.metagenome_otu_tables:
            metagenomes.add_otu_table(open(table))
        app = appraiser.appraise(genome_otu_table_collection=genomes,
                                 metagenome_otu_table_collection=metagenomes,
                                 cluster_identity=(args.cluster_id if args.cluster else None))
        
        if args.accounted_for_otu_table:
            accounted_for_otu_table_io = open(args.accounted_for_otu_table,'w')
        else:
            accounted_for_otu_table_io = None
        if args.unaccounted_for_otu_table:
            unaccounted_for_otu_table_io = open(args.unaccounted_for_otu_table,'w')
        else:
            unaccounted_for_otu_table_io = None
            
        appraiser.print_appraisal(app,
                                  accounted_for_otu_table_io=accounted_for_otu_table_io,
                                  unaccounted_for_otu_table_io=unaccounted_for_otu_table_io)
        if args.accounted_for_otu_table: accounted_for_otu_table_io.close()
        if args.unaccounted_for_otu_table: unaccounted_for_otu_table_io.close()
    else:
        raise Exception("Programming error")




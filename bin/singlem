#!/usr/bin/env python

import tempfile
import subprocess
from singlem.strain_summariser import StrainSummariser

__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2015"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3"
__maintainer__ = "Ben Woodcroft"
__email__ = "b.woodcroft near uq.edu.au"
__status__ = "Development"

import argparse
import logging
import sys
import os

try:
    import singlem.singlem
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)),'..'))
from singlem.singlem import MetagenomeOtuFinder, \
    SeqReader
import singlem.sequence_database
import singlem.query_formatters
import singlem.pipe
from singlem.summariser import Summariser
from singlem.query_formatters import NameSequenceQueryDefinition,\
    NamedQueryDefinition



def seqs(args):
    # Read in the fasta Alignment
    protein_alignment = SeqReader().protein_alignment_from_alignment_file(args.alignment)
    logging.info("Read in %i aligned protein sequences e.g. %s %s" % (len(protein_alignment),
                                                          protein_alignment[0].name,
                                                          protein_alignment[0].seq
                                                          ))

    # Read in the original nucleotide sequences
    nucleotide_sequences = SeqReader().read_nucleotide_sequences(args.reads)
    eg_name = nucleotide_sequences.keys()[0]
    logging.info("Read in %i nucleotide sequences e.g. %s %s" % (len(nucleotide_sequences),
                                                          eg_name,
                                                          nucleotide_sequences[eg_name]
                                                          ))
    if args.start_position:
        best_position = args.start_position - 1
    else:
        best_position = None
    aligned_sequences = MetagenomeOtuFinder().find_windowed_sequences(protein_alignment,
                                                nucleotide_sequences,
                                                args.window_size,
                                                best_position)
    logging.info("Printing %i aligned sequences" % len(aligned_sequences))
    print '\n'.join((s.aligned_sequence for s in aligned_sequences))



def query(args):
    db = singlem.sequence_database.SequenceDatabase.acquire(args.db)
    query_sequence = args.query_sequence
    max_target_seqs = args.max_hits
    max_divergence = args.max_divergence
    output_style = args.otu_table_type
    query_otu_table = args.query_otu_table
    query_fasta = args.query_fasta
    if (query_otu_table and query_sequence) or \
        (query_otu_table and query_fasta) or \
        (query_sequence and query_fasta):
        raise Exception("Only one of --query_fasta, --query_otu_table and --query_sequence is allowable")
    
    if query_sequence:
        query_names = ['unnamed_sequence']
        query_sequences = [query_sequence]
    elif query_otu_table:
        query_names = []
        query_sequences = []
        for line in open(query_otu_table):
            gpkg, sample, sequence = line.split("\t")[:3]
            query_sequences.append(sequence)
            query_names.append(';'.join([sample,gpkg]))
    elif query_fasta:
        query_names = []
        query_sequences = []
        for name, seq, _ in SeqReader().readfq(open(query_fasta)):
            query_names.append(name)
            query_sequences.append(seq)
    else:
        raise Exception("No query option specified, cannot continue")
    
    # blast the query against the database, output as csv
    found_distances_and_names = []
    found_query_names = []
    found_query_sequences = []
    with tempfile.NamedTemporaryFile(prefix='singlem_query') as infile:
        for i, sequence in enumerate(query_sequences):
            infile.write(">%i\n" % i)
            infile.write(sequence+"\n")
        infile.flush()
        
        cmd = "blastn -task blastn -query '%s' -db '%s' -outfmt 6 -max_target_seqs %i" %\
            (infile.name, db.sequences_fasta_file, max_target_seqs)
        logging.debug("Running cmd %s" % cmd)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        
        last_index = None
        last_differences_and_names = []
        for line in iter(proc.stdout.readline,''):
            qseqid, sseqid, _, _, mismatch, gapopen, qstart, qend = line.strip().split("\t")[:8]
            index = int(qseqid)
            if last_index is None:
                # first run through loop
                last_index = index
            elif index != last_index:
                # changed query sequences, move to the next one
                found_query_names.append(query_names[last_index])
                found_query_sequences.append(query_sequences[last_index])
                found_distances_and_names.append(last_differences_and_names)
                last_index = index
                last_differences_and_names = []
            
            #TODO: check we haven't come up against the max_target_seqs barrier
            query_length = len(query_sequences[index].replace('-',''))
            subject = db.extract_sequence_by_sseqid(sseqid)
            divergence = int(mismatch) + int(gapopen) + (int(qstart)-1) + (query_length-int(qend))
            if divergence <= max_divergence:
                last_differences_and_names.append([divergence, subject])
                
        if last_index is not None:
            # finish off the last chunk
            found_query_names.append(query_names[last_index])
            found_query_sequences.append(query_sequences[last_index])
            found_distances_and_names.append(last_differences_and_names)
        
    if query_fasta:
        namedef = NamedQueryDefinition(found_query_names)
    else:
        namedef = NameSequenceQueryDefinition(found_query_names, found_query_sequences)
    
    if output_style == 'sparse':
        formatter = singlem.query_formatters.SparseResultFormatter(namedef, found_distances_and_names)
    elif output_style == 'dense':
        formatter = singlem.query_formatters.DenseResultFormatter(namedef, found_distances_and_names)
    else:
        raise Exception()
    formatter.write(sys.stdout)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--debug', help='output debug information', action="store_true")
    parser.add_argument('--version', help='output version information and quit',  action='version', version=singlem.__version__)
    parser.add_argument('--quiet', help='only output errors', action="store_true")
    subparsers = parser.add_subparsers(title="Sub-commands", dest='subparser_name')

    temp_description = 'From raw reads to OTU table'
    pipe_parser = subparsers.add_parser('pipe',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    pipe_parser.add_argument('--sequences', nargs='+', metavar='sequence_file', help='forward nucleotide sequences to be searched')
    pipe_parser.add_argument('--threads', type=int, metavar='num_threads', help='number of CPUS to use', default=1)
    pipe_parser.add_argument('--otu_table', metavar='filename', help='output OTU table', required=True)
    pipe_parser.add_argument('--working_directory', metavar='directory', help='work in this directory (default: a temporary directory)')
    pipe_parser.add_argument('--force', action='store_true', help='overwrite working directory if required')
    pipe_parser.add_argument('--assignment_method', choices=('pplacer','diamond'), help='method of assigning taxonomy to sequences', default='pplacer')
    pipe_parser.add_argument('--evalue', help='GraftM e=value cutoff')
    pipe_parser.add_argument('--restrict_read_length', metavar='length', help='Only use this many base pairs at the start of each sequence searched (default: no restriction)', type=int)
    pipe_parser.add_argument('--previous_graftm_search_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--previous_graftm_separate_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--output_extras', action='store_true', help='give extra output for each sequence identified (mainly for debug purposes)', default=False)
    pipe_parser.add_argument('--bootstrap_contigs', nargs='+', help='assembled contigs/scaffolds to help the HMM find hits in reads (default: do not use)')
    pipe_parser.add_argument('--known_otu_tables', nargs='+', help='OTU tables previously generated that trusted taxonomies for each sequence (default: do not use)')

    temp_description = 'From protein alignment to aligned nucleotide sequences'
    seqs_parser = subparsers.add_parser('seqs',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    seqs_parser.add_argument('--alignment', metavar='aligned_fasta', help="Protein sequences hmmaligned and converted to fasta format with seqmagick", required=True)
    seqs_parser.add_argument('--reads', metavar='raw_reads', help='Unaligned nucleotide sequences that were translated into the protein sequences', required=True)
    seqs_parser.add_argument('--window_size', metavar='aa', help='Number of amino acids to use in continuous window', default=20, type=int)
    seqs_parser.add_argument('--start_position', metavar='bp', help='Start the window at the position in the alignment (1-based index) [default: pick one automatically]', type=int)
    
    temp_description = 'From aligned nucleotide sequences to searchable database'
    makedb_parser = subparsers.add_parser('makedb',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    makedb_parser.add_argument('--otu_table', help="Output from 'pipe' mode", required=True)
    makedb_parser.add_argument('--db_path', help="Name of database to create e.g. tundra.sdb", required=True)
    
    temp_description = 'Find closely related sequences in a singlem database'
    makedb_parser = subparsers.add_parser('query',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    makedb_parser.add_argument('--query_sequence', metavar='sequence', help="Sequence to use as a query")
    makedb_parser.add_argument('--query_otu_table', metavar='file', help="Query the database with all sequences in this OTU table")
    makedb_parser.add_argument('--query_fasta', metavar='file', help="Query the database with all sequences in this FASTA file")
    makedb_parser.add_argument('--db', help="Output from 'makedb' mode", required=True)
    makedb_parser.add_argument('--otu_table_type', help="Style of output table", default='sparse', choices=['dense','sparse'])
    makedb_parser.add_argument('--max_divergence', metavar='INT', help="Report sequences less than or equal to this divergence", default=4, type=int)
    makedb_parser.add_argument('--max_hits', help="--max_target_seqs parameter for blast", default=500, type=int)
    
    temp_description = 'Summarise an OTU table'
    summarise_parser = subparsers.add_parser('summarise',
                                        description=temp_description,
                                        help=temp_description,
                                        epilog=__author__)
    summarise_parser.add_argument('--otu_table', help="Output from 'pipe' mode", required=True)
    summarise_parser.add_argument('--krona', help="Name of krona file to generate")
    summarise_parser.add_argument('--strain_overview_table', help="Name of output strains table to generate")
    
    args = parser.parse_args()
    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    if args.subparser_name == 'seqs':
        seqs(args)
    elif args.subparser_name=='pipe':
        singlem.pipe.SearchPipe().run(
            sequences = args.sequences,
            otu_table = args.otu_table,
            threads = args.threads,
            bootstrap_contigs = args.bootstrap_contigs,
            known_otu_tables = args.known_otu_tables,
            assignment_method = args.assignment_method,
            evalue = args.evalue,
            restrict_read_length = args.restrict_read_length,
            output_extras = args.output_extras,
            working_directory = args.working_directory,
            force = args.force,
            previous_graftm_search_directory = args.previous_graftm_search_directory,
            previous_graftm_separate_directory = args.previous_graftm_separate_directory)

    elif args.subparser_name=='makedb':
        singlem.sequence_database.SequenceDatabase.create_from_otu_table\
            (args.db_path, open(args.otu_table))
    elif args.subparser_name=='query':
        query(args)
    elif args.subparser_name == 'summarise':
        if args.krona and args.strain_overview_table:
            raise Exception("Only one of --krona and --strain_overview_table can be generated")
        if args.krona is None and args.strain_overview_table is None:
            raise Exception("One of --krona and --strain_overview_table must be generated")
        if args.krona:
            Summariser.summarise(
                otu_table = args.otu_table,
                krona_output_prefix = args.krona)
        else:
            StrainSummariser.summarise_strains(
                otu_table_io = open(args.otu_table),
                output_table = open(args.strain_overview_table,'w'))
    else:
        raise Exception()




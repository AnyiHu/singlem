#!/usr/bin/env python

import tempfile
import re
import shutil
from string import split
import itertools
import subprocess

__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2015"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3"
__maintainer__ = "Ben Woodcroft"
__email__ = "b.woodcroft near uq.edu.au"
__status__ = "Development"

import argparse
import logging
import sys
import os
import tempdir
import extern

try:
    import singlem.singlem
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)),'..'))
from singlem.singlem import MetagenomeOtuFinder, AlignedProteinSequence, \
    HmmDatabase, SeqReader, TaxonomyFile
import singlem
import singlem.sequence_database
import singlem.query_formatters
from singlem.query_formatters import NameSequenceQueryDefinition,\
    NamedQueryDefinition
from singlem.known_otu_table import KnownOtuTable
from singlem import known_otu_table



def seqs(args):
    # Read in the fasta Alignment
    protein_alignment = protein_alignment_from_alignment_file(args.alignment)
    logging.info("Read in %i aligned protein sequences e.g. %s %s" % (len(protein_alignment),
                                                          protein_alignment[0].name,
                                                          protein_alignment[0].seq
                                                          ))

    # Read in the original nucleotide sequences
    nucleotide_sequences = read_nucleotide_sequences(args.reads)
    eg_name = nucleotide_sequences.keys()[0]
    logging.info("Read in %i nucleotide sequences e.g. %s %s" % (len(nucleotide_sequences),
                                                          eg_name,
                                                          nucleotide_sequences[eg_name]
                                                          ))
    if args.start_position:
        best_position = args.start_position - 1
    else:
        best_position = None
    aligned_sequences = MetagenomeOtuFinder().find_windowed_sequences(protein_alignment,
                                                nucleotide_sequences,
                                                args.window_size,
                                                best_position)
    logging.info("Printing %i aligned sequences" % len(aligned_sequences))
    print '\n'.join((s.aligned_sequence for s in aligned_sequences))


def protein_alignment_from_alignment_file(alignment_file):
    protein_alignment = []
    for name, seq, _ in SeqReader().readfq(open(alignment_file)):
        protein_alignment.append(AlignedProteinSequence(name, seq))
    if len(protein_alignment) > 0:
        logging.info("Read in %i aligned protein sequences e.g. %s %s" % (len(protein_alignment),
                                                          protein_alignment[0].name,
                                                          protein_alignment[0].seq
                                                          ))
    else:
        logging.info("No aligned proteins found for this HMM")
    return protein_alignment

def read_nucleotide_sequences(nucleotide_file):
    nucleotide_sequences = {}
    for name, seq, _ in SeqReader().readfq(open(nucleotide_file)):
        nucleotide_sequences[name] = seq
    return nucleotide_sequences


def align_proteins_to_hmm(proteins_file, hmm_file):
    '''hmmalign proteins to hmm, and return an alignment object'''
    
    with tempfile.NamedTemporaryFile(prefix="singlem", suffix=".fasta") as f:
        cmd = "hmmalign %s %s |seqmagick convert --input-format stockholm - %s" % (hmm_file,
                                                          proteins_file,
                                                          f.name)
        logging.debug("Running cmd: %s" % cmd)
        extern.run(cmd)
        return protein_alignment_from_alignment_file(f.name)

def pipe(args):
    hmms = HmmDatabase()
    forward_read_files = args.sequences
    output_otu_table = args.otu_table
    num_threads = args.threads
    bootstrap_contigs = args.bootstrap_contigs
    known_otu_tables = args.known_otu_tables
    graftm_assignment_method = args.assignment_method
    
    using_temporary_working_directory = args.working_directory is None
    if using_temporary_working_directory:
        tmp = tempdir.TempDir()
        working_directory = tmp.name
    else:
        working_directory = args.working_directory
        if os.path.exists(working_directory):
            if args.force:
                logging.info("Overwriting directory %s" % working_directory)
                shutil.rmtree(working_directory)
            else:
                raise Exception("Working directory '%s' already exists, not continuing" % working_directory)
        os.mkdir(working_directory)
    logging.debug("Using working directory %s" % working_directory)
    
    def return_cleanly():
        if using_temporary_working_directory: tmp.dissolve()
        
    if bootstrap_contigs:
        # Create HMMs from each of the search HMMs based on the given contigs
        bootstrap_hmms = {}
        logging.info("Generating HMMs by bootstrap..")
        # TOOD: this can be combined into a single run over the data with
        # orfm, this is a bit wasteful going over it 3 times. Instead
        # best to interface with graftm directly, I guess.
        for hmm in hmms:
            bootstrap_hmm_file = os.path.join(working_directory,
                          "bootstrap_%s" % os.path.basename(hmm.hmm_filename))
            bootstrap_hmms[hmm.hmm_filename] = bootstrap_hmm_file
            logging.debug("Finding bootstrap hmm %s" % bootstrap_hmm_file)
            cmd = "graftM bootstrap --contigs %s --search_hmm_files %s "\
                " --verbosity 2 --output_hmm %s" %(\
                  ' '.join(bootstrap_contigs),
                  hmm.hmm_path(),
                  bootstrap_hmm_file)
            logging.debug("Running cmd: %s" % cmd)
            extern.run(cmd)
    
    if not args.previous_graftm_search_directory:
        graftm_search_directory = os.path.join(working_directory, 'graftm_search')
    
        # run graftm across all the HMMs
        logging.info("Using as input %i different forward read sets e.g. %s" % (len(forward_read_files),
                                                                            forward_read_files[0]))
        cmd = "graftM graft --threads %i --forward %s "\
            "--search_hmm_files %s --search_and_align_only "\
            "--output_directory %s --aln_hmm_file %s --verbosity 2 "\
            "--input_sequence_type nucleotide"\
                             % (num_threads,
                                ' '.join(forward_read_files),
                                ' '.join(hmms.hmm_paths()),
                                graftm_search_directory,
                                hmms.hmm_paths()[0])
        if bootstrap_contigs:
            cmd += " --search_hmm_files %s" % ' '.join(
                itertools.chain(
                    (f for f in bootstrap_hmms.values() if os.path.isfile(f)),
                    hmms.hmm_paths()))
        logging.info("Running GraftM to find particular reads..")
        logging.debug("Running cmd %s" % cmd)
        extern.run(cmd)
        logging.info("Finished running GraftM search phase")
    else:
        graftm_search_directory = args.previous_graftm_search_directory
        logging.info("Using existing graftM directory %s" % args.previous_graftm_search_directory)

    # Get the names of the samples from the graftm directory
    sample_names = [f for f in os.listdir(graftm_search_directory) \
                    if os.path.isdir(os.path.join(graftm_search_directory, f))]
    logging.debug("Recovered %i samples from graftm search output e.g. %s" \
                 % (len(sample_names), sample_names[0]))

    # runs graftm for each of the HMMs doing the actual alignments, for each
    # of the input sequences
    if args.previous_graftm_placement_directory:
        graftm_align_directory_base = args.previous_graftm_placement_directory
    else:
        graftm_align_directory_base = os.path.join(working_directory, 'graftm_aligns')
        os.mkdir(graftm_align_directory_base)

        with tempfile.NamedTemporaryFile() as samples_file:
            # Don't look at samples that have no hits
            viable_sample_names = []
            for sample_name in sample_names:
                if os.stat("%s/%s/%s_hits.fa" % (graftm_search_directory,
                                              sample_name,
                                              sample_name)).st_size > 0:
                    viable_sample_names.append(sample_name)
                else:
                    logging.warn("Sample '%s' does not appear to contain any hits" % sample_name)
            if len(viable_sample_names) == 0:
                logging.info("No reads identified in any samples, stopping")
                return_cleanly()
                return
            else:
                logging.debug("Found %i samples with reads identified" % 
                              len(viable_sample_names))
            samples_file.write("\n".join(viable_sample_names))
            samples_file.flush()
            
            logging.info("Running alignments/placement in GraftM..")
            commands = []
            for sample_name in viable_sample_names:
                for hmm in hmms:
                    cmd = "graftM graft --threads %i --verbosity 2 "\
                         "--forward %s/%s/%s_hits.fa "\
                         "--graftm_package %s --output_directory %s/%s_vs_%s "\
                         "--input_sequence_type nucleotide "\
                         "--assignment_method %s" % (\
                                1, #use 1 thread since most likely better to parallelise processes with extern, not threads here
                                graftm_search_directory,
                                sample_name,
                                sample_name,
                                hmm.gpkg_path,
                                graftm_align_directory_base,
                                sample_name,
                                os.path.basename(hmm.gpkg_path),
                                graftm_assignment_method)
                    if bootstrap_contigs:
                        bootstrap_hmm = bootstrap_hmms[hmm.hmm_filename]
                        if os.path.isfile(bootstrap_hmm):
                            cmd += " --search_hmm_files %s %s" % (
                                        bootstrap_hmm,
                                        hmm.hmm_path())
                    commands.append(cmd)
            extern.run_many(commands, num_threads=args.threads)


    # get the sequences out for each of them
    with open(output_otu_table,'w') as output:
        to_print = split('gene sample sequence num_hits coverage taxonomy')
        if args.output_extras:
            to_print.append('read_names')
            to_print.append('nucleotides_aligned')
            if known_otu_tables:
                to_print.append("taxonomy_by_known?")
        output.write("\t".join(to_print)+"\n")
        
        if known_otu_tables:
            logging.info("Parsing known taxonomy OTU tables")
            known_taxes = KnownOtuTable()
            known_taxes.parse_otu_tables(known_otu_tables)
        
        un_hitify_re = re.compile(r'(.*)_vs_(.*)')
        for f in os.listdir(graftm_align_directory_base):
            if not os.path.isdir(os.path.join(graftm_align_directory_base,f)): continue
            unhitted = un_hitify_re.match(f)
            if not unhitted:
                raise Exception("Unexpected graftm sequence name %s" % f)
            sample_name = unhitted.groups(0)[0]
            hmm_basename = unhitted.groups(0)[1]

            hits_name = "%s_hits" % sample_name
            base_dir = os.path.join(graftm_align_directory_base,
                                          f,
                                          hits_name)
            
            # Need to re-align the sequences because GraftM removes columns
            # from the alignment, and we need all of them to be able to align
            # with nucleotide sequences
            proteins_file = os.path.join(base_dir, "%s_hits_orf.fa" % sample_name)
            protein_alignment = align_proteins_to_hmm(proteins_file,
                                                      hmms.hmms_and_positions[hmm_basename].hmm_path()
                                                      )
            if len(protein_alignment) == 0:
                logging.debug("Found no alignments for %s, skipping to next sample/hmm" % hmm_basename)
                continue
            nucleotide_file = os.path.join(base_dir, "%s_hits_hits.fa" % sample_name)
            nucleotide_sequences = read_nucleotide_sequences(nucleotide_file)
            taxonomies = TaxonomyFile(os.path.join(base_dir, "%s_hits_read_tax.tsv" % sample_name))

            aligned_seqs = MetagenomeOtuFinder().find_windowed_sequences(protein_alignment,
                                                    nucleotide_sequences,
                                                    20,
                                                    hmms.hmms_and_positions[hmm_basename].best_position)
            logging.debug("Found %i sequences for hmm %s, sample '%s'" % (len(aligned_seqs),
                                                                        hmm_basename,
                                                                        sample_name))
            
            # convert to OTU table, output
            for info in seqs_to_counts_and_taxonomy(aligned_seqs,
                                                    taxonomies):
                if known_otu_table:
                    tax_assigned_through_known = False
                to_print = [hmm_basename,
                                sample_name,
                                info.seq,
                                str(info.count),
                                "%.2f" % info.coverage]
                if known_otu_tables and info.seq in known_taxes:
                    tax_assigned_through_known = True
                    to_print.append(known_taxes[info.seq].taxonomy)
                else:
                    to_print.append(info.taxonomy)
                if args.output_extras:
                    to_print.append(' '.join(info.names))
                    to_print.append(' '.join([str(l) for l in info.aligned_lengths]))
                    if known_otu_tables:
                        to_print.append(tax_assigned_through_known)
                output.write("\t".join(to_print) + "\n")
                
    return_cleanly()

def seqs_to_counts_and_taxonomy(sequences, taxonomies):
    '''given an array of Sequence objects, and hash of taxonomy file,
    yield over 'Info' objects that contain e.g. the counts of the aggregated
    sequences and corresponding median taxonomies
    '''
    class CollectedInfo:
        def __init__(self):
            self.count = 0
            self.taxonomies = []
            self.names = []
            self.coverage = 0.0
            self.aligned_lengths = []
    
    seq_to_collected_info = {}
    for s in sequences:
        tax = taxonomies[s.name]
        try:
            collected_info = seq_to_collected_info[s.aligned_sequence]
        except KeyError:
            collected_info = CollectedInfo()
            seq_to_collected_info[s.aligned_sequence] = collected_info
            
        collected_info.count += 1
        collected_info.taxonomies.append(tax)
        collected_info.names.append(s.name)
        collected_info.coverage += s.coverage_increment()
        collected_info.aligned_lengths.append(s.aligned_length)
    
    class Info:
        def __init__(self, seq, count, taxonomy, names, coverage, aligned_lengths):
            self.seq = seq
            self.count = count
            self.taxonomy = taxonomy
            self.names = names
            self.coverage = coverage
            self.aligned_lengths = aligned_lengths
        
    for seq, collected_info in seq_to_collected_info.iteritems():
        yield Info(seq,
                   collected_info.count,
                   median_taxonomy(collected_info.taxonomies),
                   collected_info.names,
                   collected_info.coverage,
                   collected_info.aligned_lengths)

def median_taxonomy(taxonomies):
    levels_to_counts = []
    for tax_string in taxonomies:
        for i, tax in enumerate(tax_string.split('; ')):
            if i >= len(levels_to_counts):
                levels_to_counts.append({})
            try:
                levels_to_counts[i][tax] += 1
            except KeyError:
                levels_to_counts[i][tax] = 1
                
                
    median_tax = []
    for level_counts in levels_to_counts:
        max_count = 0
        max_tax = None
        for tax, count in level_counts.iteritems():
            if count > max_count:
                max_count = count
                max_tax = tax
        if float(max_count) / len(taxonomies) >= 0.5:
            median_tax.append(max_tax)
        else:
            break
    return '; '.join(median_tax)
    
def query(args):
    db = singlem.sequence_database.SequenceDatabase.acquire(args.db)
    query_sequence = args.query_sequence
    max_target_seqs = args.max_hits
    max_divergence = args.max_divergence
    output_style = args.otu_table_type
    query_otu_table = args.query_otu_table
    query_fasta = args.query_fasta
    if (query_otu_table and query_sequence) or \
        (query_otu_table and query_fasta) or \
        (query_sequence and query_fasta):
        raise Exception("Only one of --query_fasta, --query_otu_table and --query_sequence is allowable")
    
    if query_sequence:
        query_names = ['unnamed_sequence']
        query_sequences = [query_sequence]
    elif query_otu_table:
        query_names = []
        query_sequences = []
        for line in open(query_otu_table):
            gpkg, sample, sequence = line.split("\t")[:3]
            query_sequences.append(sequence)
            query_names.append(';'.join([sample,gpkg]))
    elif query_fasta:
        query_names = []
        query_sequences = []
        for name, seq, _ in SeqReader().readfq(open(query_fasta)):
            query_names.append(name)
            query_sequences.append(seq)
    else:
        raise Exception("No query option specified, cannot continue")
    
    # blast the query against the database, output as csv
    found_distances_and_names = []
    found_query_names = []
    found_query_sequences = []
    with tempfile.NamedTemporaryFile(prefix='singlem_query') as infile:
        for i, sequence in enumerate(query_sequences):
            infile.write(">%i\n" % i)
            infile.write(sequence+"\n")
        infile.flush()
        
        cmd = "blastn -task blastn -query '%s' -db '%s' -outfmt 6 -max_target_seqs %i" %\
            (infile.name, db.sequences_fasta_file, max_target_seqs)
        logging.debug("Running cmd %s" % cmd)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        
        last_index = None
        last_differences_and_names = []
        for line in iter(proc.stdout.readline,''):
            qseqid, sseqid, _, _, mismatch, gapopen, qstart, qend = line.strip().split("\t")[:8]
            index = int(qseqid)
            if last_index is None:
                # first run through loop
                last_index = index
            elif index != last_index:
                # changed query sequences, move to the next one
                found_query_names.append(query_names[last_index])
                found_query_sequences.append(query_sequences[last_index])
                found_distances_and_names.append(last_differences_and_names)
                last_index = index
                last_differences_and_names = []
            
            #TODO: check we haven't come up against the max_target_seqs barrier
            query_length = len(query_sequences[index].replace('-',''))
            subject = db.extract_sequence_by_sseqid(sseqid)
            divergence = int(mismatch) + int(gapopen) + (int(qstart)-1) + (query_length-int(qend))
            if divergence <= max_divergence:
                last_differences_and_names.append([divergence, subject])
                
        if last_index is not None:
            # finish off the last chunk
            found_query_names.append(query_names[last_index])
            found_query_sequences.append(query_sequences[last_index])
            found_distances_and_names.append(last_differences_and_names)
        
    if query_fasta:
        namedef = NamedQueryDefinition(found_query_names)
    else:
        namedef = NameSequenceQueryDefinition(found_query_names, found_query_sequences)
    
    if output_style == 'sparse':
        formatter = singlem.query_formatters.SparseResultFormatter(namedef, found_distances_and_names)
    elif output_style == 'dense':
        formatter = singlem.query_formatters.DenseResultFormatter(namedef, found_distances_and_names)
    else:
        raise Exception()
    formatter.write(sys.stdout)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--debug', help='output debug information', action="store_true")
    parser.add_argument('--version', help='output version information and quit',  action='version', version=singlem.__version__)
    parser.add_argument('--quiet', help='only output errors', action="store_true")
    subparsers = parser.add_subparsers(help="--", dest='subparser_name')

    pipe_parser = subparsers.add_parser('pipe',
                                        description='From raw reads to OTU table',
                                        epilog=__author__)
    pipe_parser.add_argument('--sequences', nargs='+', metavar='sequence_file', help='forward nucleotide sequences to be searched')
    pipe_parser.add_argument('--threads', type=int, metavar='num_threads', help='number of CPUS to use', default=1)
    pipe_parser.add_argument('--otu_table', metavar='filename', help='output OTU table', required=True)
    pipe_parser.add_argument('--working_directory', metavar='directory', help='work in this directory (default: a temporary directory)')
    pipe_parser.add_argument('--force', action='store_true', help='overwrite working directory if required')
    pipe_parser.add_argument('--assignment_method', choices=('pplacer','diamond'), help='method of assigning taxonomy to sequences', default='pplacer')
    pipe_parser.add_argument('--previous_graftm_search_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--previous_graftm_placement_directory', metavar='directory', help='previously half-run pipeline (mainly for debug purposes)')
    pipe_parser.add_argument('--output_extras', action='store_true', help='give extra output for each sequence identified (mainly for debug purposes)', default=False)
    pipe_parser.add_argument('--bootstrap_contigs', nargs='+', help='assembled contigs/scaffolds to help the HMM find hits in reads (default: do not use)')
    pipe_parser.add_argument('--known_otu_tables', nargs='+', help='OTU tables previously generated that trusted taxonomies for each sequence (default: do not use)')

    seqs_parser = subparsers.add_parser('seqs',
                                        description='From protein alignment to aligned nucleotide sequences',
                                        epilog=__author__)
    seqs_parser.add_argument('--alignment', metavar='aligned_fasta', help="Protein sequences hmmaligned and converted to fasta format with seqmagick", required=True)
    seqs_parser.add_argument('--reads', metavar='raw_reads', help='Unaligned nucleotide sequences that were translated into the protein sequences', required=True)
    seqs_parser.add_argument('--window_size', metavar='aa', help='Number of amino acids to use in continuous window', default=20, type=int)
    seqs_parser.add_argument('--start_position', metavar='bp', help='Start the window at the position in the alignment (1-based index) [default: pick one automatically]', type=int)
    
    makedb_parser = subparsers.add_parser('makedb',
                                        description='From aligned nucleotide sequences to searchable database',
                                        epilog=__author__)
    makedb_parser.add_argument('--otu_table', help="Output from 'pipe' mode", required=True)
    makedb_parser.add_argument('--db_path', help="Name of database to create e.g. tundra.sdb", required=True)
    
    makedb_parser = subparsers.add_parser('query',
                                        description='Find closely related sequences in a singlem database',
                                        epilog=__author__)
    makedb_parser.add_argument('--query_sequence', metavar='sequence', help="Sequence to use as a query")
    makedb_parser.add_argument('--query_otu_table', metavar='file', help="Query the database with all sequences in this OTU table")
    makedb_parser.add_argument('--query_fasta', metavar='file', help="Query the database with all sequences in this FASTA file")
    makedb_parser.add_argument('--db', help="Output from 'makedb' mode", required=True)
    makedb_parser.add_argument('--otu_table_type', help="Style of output table", default='sparse', choices=['dense','sparse'])
    makedb_parser.add_argument('--max_divergence', metavar='INT', help="Report sequences less than or equal to this divergence", default=4, type=int)
    makedb_parser.add_argument('--max_hits', help="--max_target_seqs parameter for blast", default=500, type=int)
    

    args = parser.parse_args()
    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    if args.subparser_name == 'seqs':
        seqs(args)
    elif args.subparser_name=='pipe':
        pipe(args)
    elif args.subparser_name=='makedb':
        singlem.sequence_database.SequenceDatabase.create_from_otu_table\
            (args.db_path, open(args.otu_table))
    elif args.subparser_name=='query':
        query(args)
    else:
        raise Exception()














